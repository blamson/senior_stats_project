---
title: "Exploratory Data Analysis"
author: "Brady Lamson"
format: html
editor: visual
---

```{r}
library(dplyr)
library(skimr)
library(readxl)
library(glue)
library(ggplot2)
library(tidyr)
source('helpers.R')
```

```{r}
data_path <- "data/Pompeii data 2023 for stats .xlsx"
sheets <- readxl::excel_sheets(data_path)
print(sheets)

for (sheet in sheets) {
    data <- readxl::read_excel(data_path, sheet = sheet)
    print(glue::glue(
        "
        ---
        Sheet name: {sheet}
        Column count: {ncol(data)}
        Row count: {nrow(data)}
        "
    ))
    rm(data)
}
```

We'll be first exploring the "all peristyles" sheet as it contains a ridiculous number of columns.

```{r}
df <- read_excel(data_path, sheet = "all peristyles")
```

Below is the set of column names for this dataset.

```{r}
colnames(df)
```

That's a LOT of columns. To help cut down on time for columns we definitely can't use, let's first look at how many have missing values and how many there are. First let's clean up the column names to make them easier to work with. Things like spaces and the percent sign can make certain operations difficult.

```{r}
df <- fix_variable_names(df)
```

```{r}
colnames(df)
```

```{r}
overview <- skim(df)
overview
```

Number and name will be combined here to generate a unique id per observation.

```{r}
df <- tidyr::unite(df, col='id', c('number', 'name'), sep='_')
```

Poking through this output shows a huge variation in missing values. Let's look a bit closer.

```{r}
na_count <- completeness_by_var(df)

na_count
```

```{r}
na_count %>%
    ggplot(
        aes(
            x = completeness * 100, 
            y=after_stat(count)/sum(after_stat(count)) * 100
        )
    ) +
    geom_histogram(binwidth=10, color="black", fill="grey", boundary=0) +
    scale_x_continuous(breaks = seq(from=0, to=100, by=20)) +
    ggtitle("Distribution of completeness in 'All Peristyles' sheet") +
    labs(x="Completeness (%)", y="Relative Frequency (%)") +
    theme_linedraw(base_line_size = 1)
```

What we can see here is the vast majority of the data set has a lot of NAs. Only a touch over 10% is at least 80% complete. Thankfully that still leaves us with a ton of columns to work with, so we'll use that as our starting threshold.

```{r}
df_complete_vars <- filter_by_completeness(df, threshold = .8)
df_complete_vars
```

# Column Notes

```{r}
skim(df_complete_vars)
```

## General notes

We shouldn't take these skim results entirely at face value. In particular, the `n_unique` value here differentiates between 'yes' and 'yes with notes'. Many of these columns are actually boolean when you look at them and will need to be cleaned up. As well, there are some numerical columns that got read in as categorical ones due to extra text in the cell. I believe the only ones here are `property_size`, `columns`, `pillars`. These will need to be handled with care depending on if we use them.

## Column specifics

### number

appears to be an id column. What's strange about this is it only has 207 unique values in 220 rows. Will need to look into this.

### name

Seems to represent the homes owner or its purpose. Guessing for rows labeled simply "house" have an unknown owner?

NOTE: Can probably combine both number and name to create a unique identifier column.

### peristyle_type

Confusing. It's only unique values are "tru" and "truncated". I figure both of those would correspond to the same thing?

```{r}
df_complete_vars$peristyle_type %>% unique()
```

NOTE: Based on the dissertation "tru" seems to stand for "true". Confirm this and then change the text for future ease of understanding.

### tablinum (MAYBE)

Represents [this, probably](https://en.wikipedia.org/wiki/Tablinum). The column itself a boolean variable with some notes occasionally tacked on. If we were to use this column we'd likely need to abandon anything after the "yes".

```{r}
df_complete_vars$tablinum %>% unique()
```

NOTE: Check frequency of "?" in the dataset as those are definitely missing values.

NOTE: Probably just keep the yes/no and maybe the wide/not-wide depending on frequency.

```{r}
df_complete_vars %>% filter(tablinum == "NA")
```

### pluteus (MAYBE)

is in a similar situation. Semi-boolean with occasional extra notes. Overall categories seem to be (yes, no, partial)

```{r}
df_complete_vars$pluteus %>% unique()
```

```{r}
df_complete_vars %>% filter(pluteus == "?")
```

### pillars (MAYBE)

Could be treated as a numeric column depending on how we want to use it. Will need to treat with care.

```{r}
df_complete_vars$pillars %>% unique()
```

### columns

Could be treated as a numeric variable. Possibly not useful if it corresponds to the number of columns currently standing as we're more interested in how many columns there USED to be.

```{r}
df_complete_vars$columns %>% unique()
```

### property_size 
Needs to have the "?" symbol replaced with a missing value and changed to numeric. Also need to get the units of this column. Square feet?

```{r}
df_complete_vars$property_size %>% unique()
```

### Peristyle size, Property size and % of property:

All of these are highly correlated and % of property cant exist (most of the time) without property size. Probably just stick with peristyle size if we use it.

NOTE: "House of Painters at Work" is missing these values but there is a lot of known information about this location.

### Walkway NSEW, Portico NSEW

These columns are interesting as they're supposed to be used in tandem. Missing values here are likely just 0 or unknown. Note that these values aren't just simple "NE" or "SW" style coordinates as some rows have results like "NWE". We can easily use these.

Also note, these also work off of the "portico_sides" and "walkway_sides" variables.

If we make the assumption that NA stands for 0 (which I do purely for the sake of EDA), we can see how complete the combined variable is.

```{r}
value_counts <- df %>%
    dplyr::select(walkway_N, walkway_S, walkway_E, walkway_W) %>%
    rowSums(., na.rm=TRUE)

prop <- sum(value_counts > 0) / length(value_counts)
glue::glue("When combined, the walkway data is {round(prop * 100, 2)}% complete.")
```

```{r}
df %>%
    dplyr::select(portico_N, portico_S, portico_E, portico_W)
```

```{r}
value_counts <- df %>%
    dplyr::select(portico_N, portico_S, portico_E, portico_W) %>%
    rowSums(., na.rm=TRUE)

prop <- sum(value_counts > 0) / length(value_counts)
glue::glue("When combined, the portico data is {round(prop * 100, 2)}% complete.")
```

Also note that there appear to be duplicated columns on read in. `portico_sides` for instance has two versions, 130 and 33. I would guess these correspond to their column index from the original sheet.

```{r}
df %>% 
    dplyr::select(c(portico_sides...33, portico_sides...130))
```

# Evaluating possible types of missing values

There are a few possible ways a cell may be interpreted as "missing". These will vary column to column so care must be taken to ensure a poor assumption is not made for a specific column. The ones that jump out to be from perusing the dataset are

-   A blank cell
-   NA
-   "unknown"
-   "?"

```{r}
for (string in c("?", "unknown", "NA")) {
    print(glue::glue("Examining frequency of string: <{string}>"))
    print(count_particular_string(df, string))
} 

```

Sometimes these values will exist in the same variable, like blank cells and NA for instance. Blank cells in particular we need to be very cautious about as those can sometimes correspond to 0s in boolean variables like in walkway NSEW.

# Evaluating incomplete variables

I'm particularly curious about the VERY incomplete variables, so let's take a look at the ones below 5% completeness.

```{r}
df_incomplete_vars <- filter_by_completeness(df, threshold = .05, operator = "<")
df_incomplete_vars
```
